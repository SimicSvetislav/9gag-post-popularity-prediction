{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['motorcyclist goes', 'ride']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Rake()\n",
    "\n",
    "r.extract_keywords_from_text(\"Motorcyclist goes for a ride\")\n",
    "\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show earlier', 'wish', 'discovered']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Metric, Rake\n",
    "\n",
    "r = Rake(language='english')\n",
    "\n",
    "# If you want to provide your own set of stop words and punctuations to\n",
    "r = Rake()\n",
    "\n",
    "# If you want to control the metric for ranking. Paper uses d(w)/f(w) as the\n",
    "# metric. You can use this API with the following metrics:\n",
    "# 1. d(w)/f(w) (Default metric) Ratio of degree of word to its frequency.\n",
    "# 2. d(w) Degree of word only.\n",
    "# 3. f(w) Frequency of word only.\n",
    "\n",
    "r = Rake(ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO)\n",
    "r = Rake(ranking_metric=Metric.WORD_DEGREE)\n",
    "r = Rake(ranking_metric=Metric.WORD_FREQUENCY)\n",
    "\n",
    "# If you want to control the max or min words in a phrase, for it to be\n",
    "# considered for ranking you can initialize a Rake instance as below:\n",
    "\n",
    "r = Rake(min_length=1, max_length=2)\n",
    "\n",
    "r.extract_keywords_from_text(\"I wish I'd discovered this show earlier\")\n",
    "\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "Random word : being\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "\n",
    "print(\"Random word :\", lemmatizer.lemmatize(\"being\", pos = 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello', 'NNP')\n",
      "('welcome', 'NN')\n",
      "('to', 'TO')\n",
      "('the', 'DT')\n",
      "('world', 'NN')\n",
      "('of', 'IN')\n",
      "('to', 'TO')\n",
      "('learn', 'VB')\n",
      "('Categorizing', 'NNP')\n",
      "('and', 'CC')\n",
      "('POS', 'NNP')\n",
      "('Tagging', 'NNP')\n",
      "('with', 'IN')\n",
      "('NLTK', 'NNP')\n",
      "('and', 'CC')\n",
      "('Python', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = word_tokenize(\"Hello welcome to the world of to learn Categorizing and POS Tagging with NLTK and Python\")\n",
    "\n",
    "tagged = nltk.pos_tag(text)\n",
    "\n",
    "for tag in tagged:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lemmatize(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    # words = text.split()\n",
    "    \n",
    "    tokenized = word_tokenize(text)\n",
    "    \n",
    "    tagged_words = nltk.pos_tag(tokenized)\n",
    "    # print(tagged_words)\n",
    "    \n",
    "    text = \"\"\n",
    "    \n",
    "    for tagged_word in tagged_words:\n",
    "        \n",
    "        pos_tag = penn_to_wn(tagged_word[1])\n",
    "        \n",
    "        # print(pos_tag)\n",
    "        \n",
    "        if pos_tag == None:\n",
    "            text += tagged_word[0] + \" \"\n",
    "            continue\n",
    "        \n",
    "        lemmatized_word = lemmatizer.lemmatize(tagged_word[0], pos_tag)\n",
    "        \n",
    "        text += lemmatized_word + \" \"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i be an expert at master wait '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_lemmatize(\"I was an expert at master waiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL Server version  8.0.16\n",
      "You're connected to database:  ('9gag',)\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "connection = mysql.connector.connect(host='localhost',\n",
    "                                     database='9gag',\n",
    "                                     user='root',\n",
    "                                     password='root',)\n",
    "if connection.is_connected():\n",
    "    db_Info = connection.get_server_info()\n",
    "    print(\"Connected to MySQL Server version \", db_Info)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select database();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"You're connected to database: \", record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " id       object\n",
      "title    object\n",
      "dtype: object\n",
      "Number of questions,columns= (6038, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sql_query = \"SELECT id, title FROM actual_post\"\n",
    "\n",
    "df_idf=pd.read_sql(sql_query, connection)\n",
    " \n",
    "# print schema\n",
    "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
    "print(\"Number of questions,columns=\",df_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>a0Nv0Pd</td>\n",
       "      <td>Hat trick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>a0Nv1bq</td>\n",
       "      <td>What happened?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>a0Nv1jn</td>\n",
       "      <td>Do you miss the Neinties ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>a0Nv3ed</td>\n",
       "      <td>A different perspective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>a0Nv4Yv</td>\n",
       "      <td>One of the worst moments of pc building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6033</td>\n",
       "      <td>aZyqYGX</td>\n",
       "      <td>Doomguy teaches Isabelle his techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6034</td>\n",
       "      <td>aZyqYm9</td>\n",
       "      <td>Clear lemon pie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6035</td>\n",
       "      <td>aZyqYNn</td>\n",
       "      <td>This entire thing was made with color pencils!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6036</td>\n",
       "      <td>aZyqYVn</td>\n",
       "      <td>Quarantine hacks gone wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6037</td>\n",
       "      <td>aZyqZmz</td>\n",
       "      <td>Dunder-Mifflin this is Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6038 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           title\n",
       "0     a0Nv0Pd                                       Hat trick\n",
       "1     a0Nv1bq                                  What happened?\n",
       "2     a0Nv1jn                      Do you miss the Neinties ?\n",
       "3     a0Nv3ed                         A different perspective\n",
       "4     a0Nv4Yv         One of the worst moments of pc building\n",
       "...       ...                                             ...\n",
       "6033  aZyqYGX         Doomguy teaches Isabelle his techniques\n",
       "6034  aZyqYm9                                 Clear lemon pie\n",
       "6035  aZyqYNn  This entire thing was made with color pencils!\n",
       "6036  aZyqYVn                     Quarantine hacks gone wrong\n",
       "6037  aZyqZmz                   Dunder-Mifflin this is Brazil\n",
       "\n",
       "[6038 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gager detect '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf['text'] = df_idf['title']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x: preprocess_lemmatize(x))\n",
    "\n",
    "df_idf['text'][20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary and word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6038, 6009)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def read_stopwords(stopwords_file='english_stopwords.txt'):\n",
    "    \n",
    "    with open(stopwords_file, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "stopwords = read_stopwords()\n",
    "    \n",
    "docs=df_idf['text'].tolist()\n",
    "\n",
    "cv=CountVectorizer(max_df=0.25,stop_words=stopwords)\n",
    "# cv=CountVectorizer(max_df=0.25,stop_words='english') # use max_features=<limit> to limit vocabulary size\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hat',\n",
       " 'trick',\n",
       " 'happen',\n",
       " 'miss',\n",
       " 'neinties',\n",
       " 'different',\n",
       " 'perspective',\n",
       " 'one',\n",
       " 'bad',\n",
       " 'moment']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test=df_idf['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \n",
    "    if topn == -1:\n",
    "        topn = len(sorted_items)\n",
    "    \n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title:\n",
      "I wish I'd discovered this show earlier\n",
      "\n",
      "Keywords list:\n",
      "earlier 0.659\n",
      "wish 0.572\n",
      "show 0.487\n"
     ]
    }
   ],
   "source": [
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "doc=docs_test[20]\n",
    "doc = \"I wish I'd discovered this show earlier\"\n",
    " \n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    " \n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,-1)\n",
    " \n",
    "print(\"\\nTitle:\")\n",
    "print(doc)\n",
    "print(\"\\nKeywords list:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_tfidf(original, lemmatized):\n",
    "    \n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([lemmatized]))\n",
    " \n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,-1)\n",
    "    \n",
    "    # print(\"\\nTitle:\")\n",
    "    # print(original)\n",
    "    # print(\"\\nKeywords list:\")\n",
    "    # for k in keywords:\n",
    "        # print(k,keywords[k])\n",
    "        \n",
    "    return list(keywords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_tfidf_as_map(original, lemmatized):\n",
    "    \n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([lemmatized]))\n",
    " \n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,-1)\n",
    "    \n",
    "    # print(\"\\nTitle:\")\n",
    "    # print(original)\n",
    "    # print(\"\\nKeywords list:\")\n",
    "    # for k in keywords:\n",
    "        # print(k,keywords[k])\n",
    "        \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_keywords(keywords):\n",
    "    \n",
    "    keywords_with_scores = []\n",
    "    \n",
    "    for key, value in keywords.items():\n",
    "        keywords_with_scores.append(key + '|' + str(value))\n",
    "    \n",
    "    return keywords_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earlier|0.559', 'discover|0.531', 'wish|0.485', 'show|0.413']\n"
     ]
    }
   ],
   "source": [
    "title = \"I wish I'd discovered this show earlier\"\n",
    "\n",
    "lemmatized_title = preprocess_lemmatize(title)\n",
    "\n",
    "extract_keywords_tfidf(title, lemmatized_title)\n",
    "kw = extract_keywords_tfidf_as_map(title, lemmatized_title)\n",
    "\n",
    "lista = map_keywords(kw)\n",
    "\n",
    "print(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "if (connection.is_connected()):\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't write : ['a0NvBXQ', 'Michiroみちろ', 1, 'michiroみちろ|1.0']\n",
      "Can't write : ['a2WEYQY', 'Hii❤', 1, 'hii|1.0']\n",
      "Can't write : ['a7WvRDr', 'Kĕniæl, the Overseer', 2, 'overseer|0.707', 'kĕniæl|0.707']\n",
      "Can't write : ['aBm9Adx', \"ಠ_ಠ...When I read this news.I don't think they're UFOs...ಠ_ಠ\", 5, 'ಠ_ಠ|0.795', 'ufos|0.367', 'read|0.294', 'news|0.29', 'think|0.251']\n",
      "Can't write : ['aD4RNr9', 'Friends furever ♡', 2, 'furever|0.839', 'friend|0.543']\n",
      "Can't write : ['aGdKm87', 'ステイホーム', 1, 'ステイホーム|1.0']\n",
      "Can't write : ['aj9Qnr8', '¯\\\\_(ツ)_/¯ Well', 1, 'well|1.0']\n",
      "Can't write : ['an4ePOL', 'Well ¯\\\\_(ツ)_/¯', 1, 'well|1.0']\n",
      "Can't write : ['aNgm7Qv', 'Quarintine ️', 1, 'quarintine|1.0']\n",
      "Can't write : ['aqn40Lp', 'The new suprime leader is kinda cute ❤', 5, 'suprime|0.55', 'kinda|0.494', 'cute|0.419', 'leader|0.416', 'new|0.325']\n",
      "Can't write : ['aXgZ6dD', 'ᵒʰ ⁿᵒ', 2, 'ⁿᵒ|0.707', 'ᵒʰ|0.707']\n",
      "Can't write : ['aYyj0qN', 'The best of friends ♡', 2, 'friend|0.717', 'best|0.698']\n",
      "Most keywords : 28\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('keywords_v2.csv', 'w', newline='') as keywords_file:\n",
    "    \n",
    "    writer = csv.writer(keywords_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)  \n",
    "    \n",
    "    sql_query = \"SELECT id, title FROM actual_post\"\n",
    "\n",
    "    df_titles=pd.read_sql(sql_query, connection)\n",
    "    \n",
    "    most_keywords = 0\n",
    "    \n",
    "    for index, row in df_titles.iterrows():\n",
    "        # print(row['id'], row['title'])\n",
    "        \n",
    "        title = row['title']\n",
    "\n",
    "        lemmatized_title = preprocess_lemmatize(title)\n",
    "\n",
    "        keywords = extract_keywords_tfidf_as_map(title, lemmatized_title)\n",
    "        \n",
    "        if len(keywords) > most_keywords:\n",
    "            most_keywords = len(keywords)\n",
    "        \n",
    "        write_data = [row['id']]\n",
    "        write_data.append(row['title'])\n",
    "        write_data.append(len(keywords))\n",
    "        write_data.extend(map_keywords(keywords))\n",
    "        \n",
    "        try: \n",
    "            writer.writerow(write_data)\n",
    "        except:\n",
    "            print(\"Can't write :\", write_data)\n",
    "        \n",
    "    print(\"Most keywords :\", most_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            Hat trick\n",
       "1                                       What happened?\n",
       "2                           Do you miss the Neinties ?\n",
       "3                              A different perspective\n",
       "4              One of the worst moments of pc building\n",
       "                             ...                      \n",
       "6033           Doomguy teaches Isabelle his techniques\n",
       "6034                                   Clear lemon pie\n",
       "6035    This entire thing was made with color pencils!\n",
       "6036                       Quarantine hacks gone wrong\n",
       "6037                     Dunder-Mifflin this is Brazil\n",
       "Name: title, Length: 6038, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titles['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding keywords with highest sum of weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keywords_v2.csv', 'r') as keywords_file:\n",
    "    lines = list(csv.reader(keywords_file))\n",
    "    \n",
    "    counter_dict = {}\n",
    "    weigths_dict = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        words_data = line[3:]\n",
    "        \n",
    "        for word_data in words_data:\n",
    "            word, weigth = word_data.split('|')\n",
    "\n",
    "            # print(word + ' - ' + weigth)\n",
    "            \n",
    "            if word in counter_dict:\n",
    "                counter_dict[word] += 1\n",
    "                weigths_dict[word] += float(weigth)\n",
    "            else:\n",
    "                counter_dict[word] = 1\n",
    "                weigths_dict[word] = float(weigth)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most frequent keywords :\n",
      "('get', 173)\n",
      "('like', 157)\n",
      "('go', 154)\n",
      "('make', 146)\n",
      "('one', 132)\n",
      "('quarantine', 121)\n",
      "('time', 115)\n",
      "('good', 112)\n",
      "('know', 107)\n",
      "('day', 100)\n",
      "('guy', 98)\n",
      "('see', 86)\n",
      "('look', 85)\n",
      "('new', 77)\n",
      "('people', 74)\n",
      "('right', 73)\n",
      "('old', 71)\n",
      "('say', 70)\n",
      "('year', 69)\n",
      "('still', 65)\n"
     ]
    }
   ],
   "source": [
    "counter_dict_sorted = {k: v for k, v in sorted(counter_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# print(counter_dict_sorted)\n",
    "\n",
    "thresh = 20\n",
    "\n",
    "print(str(thresh) + \" most frequent keywords :\")\n",
    "\n",
    "for index, item in enumerate(counter_dict_sorted.items()):\n",
    "    \n",
    "    if index == thresh:\n",
    "        break\n",
    "    \n",
    "    print(item)\n",
    "    # print(\"'\" + item[0] + \"', \", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('like', 59.747000000000014)\n",
      "('get', 59.47200000000001)\n",
      "('go', 57.97099999999999)\n",
      "('quarantine', 54.42300000000004)\n",
      "('know', 52.456)\n",
      "('one', 50.958000000000006)\n",
      "('make', 48.033999999999985)\n",
      "('good', 47.760000000000005)\n",
      "('time', 46.27599999999999)\n",
      "('day', 39.937)\n",
      "('true', 36.94399999999999)\n",
      "('guy', 36.74299999999999)\n",
      "('see', 34.21000000000001)\n",
      "('well', 31.456999999999994)\n",
      "('look', 31.152000000000008)\n",
      "('say', 30.077)\n",
      "('right', 30.036999999999995)\n",
      "('year', 28.385999999999996)\n",
      "('new', 27.474999999999994)\n",
      "('old', 26.935999999999993)\n"
     ]
    }
   ],
   "source": [
    "weigths_dict_sorted = {k: v for k, v in sorted(weigths_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "for index, item in enumerate(weigths_dict_sorted.items()):\n",
    "    \n",
    "    if index == thresh:\n",
    "        break\n",
    "    \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_importance_dict = {}\n",
    "\n",
    "for index, item in enumerate(counter_dict_sorted.items()):\n",
    "    \n",
    "    if item[1] > 10:\n",
    "        relative_importance_dict[item[0]] = weigths_dict_sorted[item[0]] / item[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hmmm', 0.9589523809523809)\n",
      "('fact', 0.7760000000000001)\n",
      "('true', 0.7243921568627448)\n",
      "('okay', 0.7020000000000001)\n",
      "('classic', 0.7019230769230768)\n",
      "('wholesome', 0.7014705882352941)\n",
      "('doggo', 0.6958461538461538)\n",
      "('welcome', 0.6926153846153846)\n",
      "('wtf', 0.6918947368421052)\n",
      "('title', 0.68875)\n",
      "('fine', 0.6886666666666665)\n",
      "('stupid', 0.6856428571428571)\n",
      "('smile', 0.6851538461538461)\n",
      "('accurate', 0.6764375)\n",
      "('legend', 0.6712727272727274)\n",
      "('russia', 0.6705000000000002)\n",
      "('lol', 0.6596875)\n",
      "('worth', 0.6566923076923078)\n",
      "('wait', 0.6534687500000002)\n",
      "('bro', 0.6436153846153846)\n"
     ]
    }
   ],
   "source": [
    "relative_importance_sorted = {k: v for k, v in sorted(relative_importance_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "for index, item in enumerate(relative_importance_sorted.items()):\n",
    "    \n",
    "    if index == thresh:\n",
    "        break\n",
    "    \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
