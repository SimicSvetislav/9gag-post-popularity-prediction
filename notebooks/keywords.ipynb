{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sveta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['motorcyclist goes', 'ride']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Rake()\n",
    "\n",
    "r.extract_keywords_from_text(\"Motorcyclist goes for a ride\")\n",
    "\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show earlier', 'wish', 'discovered']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Metric, Rake\n",
    "\n",
    "r = Rake(language='english')\n",
    "\n",
    "# If you want to provide your own set of stop words and punctuations to\n",
    "r = Rake()\n",
    "\n",
    "# If you want to control the metric for ranking. Paper uses d(w)/f(w) as the\n",
    "# metric. You can use this API with the following metrics:\n",
    "# 1. d(w)/f(w) (Default metric) Ratio of degree of word to its frequency.\n",
    "# 2. d(w) Degree of word only.\n",
    "# 3. f(w) Frequency of word only.\n",
    "\n",
    "r = Rake(ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO)\n",
    "r = Rake(ranking_metric=Metric.WORD_DEGREE)\n",
    "r = Rake(ranking_metric=Metric.WORD_FREQUENCY)\n",
    "\n",
    "# If you want to control the max or min words in a phrase, for it to be\n",
    "# considered for ranking you can initialize a Rake instance as below:\n",
    "\n",
    "r = Rake(min_length=1, max_length=2)\n",
    "\n",
    "r.extract_keywords_from_text(\"I wish I'd discovered this show earlier\")\n",
    "\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "Random word : being\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "\n",
    "print(\"Random word :\", lemmatizer.lemmatize(\"being\", pos = 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lemmatize(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    text = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        for pos_tag in ['n', 'v', 'a', 'r']:\n",
    "            \n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos_tag)\n",
    "\n",
    "            if lemmatized_word != word:\n",
    "                # print(lemmatized_word)\n",
    "                break\n",
    "                \n",
    "        text += lemmatized_word + \" \"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i be an expert at master wait '"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_lemmatize(\"I am an expert at master waiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL Server version  8.0.16\n",
      "You're connected to database:  ('9gag',)\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "connection = mysql.connector.connect(host='localhost',\n",
    "                                     database='9gag',\n",
    "                                     user='root',\n",
    "                                     password='root',)\n",
    "if connection.is_connected():\n",
    "    db_Info = connection.get_server_info()\n",
    "    print(\"Connected to MySQL Server version \", db_Info)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"select database();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"You're connected to database: \", record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " id       object\n",
      "title    object\n",
      "dtype: object\n",
      "Number of questions,columns= (6038, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sql_query = \"SELECT id, title FROM actual_post\"\n",
    "\n",
    "df_idf=pd.read_sql(sql_query, connection)\n",
    " \n",
    "# print schema\n",
    "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
    "print(\"Number of questions,columns=\",df_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>a0Nv0Pd</td>\n",
       "      <td>Hat trick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>a0Nv1bq</td>\n",
       "      <td>What happened?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>a0Nv1jn</td>\n",
       "      <td>Do you miss the Neinties ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>a0Nv3ed</td>\n",
       "      <td>A different perspective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>a0Nv4Yv</td>\n",
       "      <td>One of the worst moments of pc building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6033</td>\n",
       "      <td>aZyqYGX</td>\n",
       "      <td>Doomguy teaches Isabelle his techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6034</td>\n",
       "      <td>aZyqYm9</td>\n",
       "      <td>Clear lemon pie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6035</td>\n",
       "      <td>aZyqYNn</td>\n",
       "      <td>This entire thing was made with color pencils!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6036</td>\n",
       "      <td>aZyqYVn</td>\n",
       "      <td>Quarantine hacks gone wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6037</td>\n",
       "      <td>aZyqZmz</td>\n",
       "      <td>Dunder-Mifflin this is Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6038 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           title\n",
       "0     a0Nv0Pd                                       Hat trick\n",
       "1     a0Nv1bq                                  What happened?\n",
       "2     a0Nv1jn                      Do you miss the Neinties ?\n",
       "3     a0Nv3ed                         A different perspective\n",
       "4     a0Nv4Yv         One of the worst moments of pc building\n",
       "...       ...                                             ...\n",
       "6033  aZyqYGX         Doomguy teaches Isabelle his techniques\n",
       "6034  aZyqYm9                                 Clear lemon pie\n",
       "6035  aZyqYNn  This entire thing was made with color pencils!\n",
       "6036  aZyqYVn                     Quarantine hacks gone wrong\n",
       "6037  aZyqZmz                   Dunder-Mifflin this is Brazil\n",
       "\n",
       "[6038 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(text):\n",
    "    \n",
    "    text=text.lower()\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gager detect '"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf['text'] = df_idf['title']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x: preprocess_lemmatize(x))\n",
    "\n",
    "df_idf['text'][20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary and word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6038, 5771)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def read_stopwords(stopwords_file='english_stopwords.txt'):\n",
    "    \n",
    "    with open(stopwords_file, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "stopwords = read_stopwords()\n",
    "    \n",
    "docs=df_idf['text'].tolist()\n",
    "\n",
    "cv=CountVectorizer(max_df=0.25,stop_words=stopwords)\n",
    "# cv=CountVectorizer(max_df=0.25,stop_words='english') # use max_features=<limit> to limit vocabulary size\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hat',\n",
       " 'trick',\n",
       " 'happen',\n",
       " 'miss',\n",
       " 'neinties',\n",
       " 'different',\n",
       " 'perspective',\n",
       " 'one',\n",
       " 'bad',\n",
       " 'moment']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test=df_idf['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \n",
    "    if topn == -1:\n",
    "        topn = len(sorted_items)\n",
    "    \n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title:\n",
      "I wish I'd discovered this show earlier\n",
      "\n",
      "Keywords list:\n",
      "wish 0.761\n",
      "show 0.648\n"
     ]
    }
   ],
   "source": [
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "doc=docs_test[20]\n",
    "doc = \"I wish I'd discovered this show earlier\"\n",
    " \n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    " \n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,-1)\n",
    " \n",
    "print(\"\\nTitle:\")\n",
    "print(doc)\n",
    "print(\"\\nKeywords list:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_tfidf(original, lemmatized):\n",
    "    \n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([lemmatized]))\n",
    " \n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,-1)\n",
    "    \n",
    "    # print(\"\\nTitle:\")\n",
    "    # print(original)\n",
    "    # print(\"\\nKeywords list:\")\n",
    "    # for k in keywords:\n",
    "        # print(k,keywords[k])\n",
    "        \n",
    "    return list(keywords.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['strike']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"He strikes again\"\n",
    "\n",
    "lemmatized_title = preprocess_lemmatize(title)\n",
    "\n",
    "extract_keywords_tfidf(title, lemmatized_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "if (connection.is_connected()):\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't write : ['a0NvBXQ', 'Michiroみちろ', 1, 'michiroみちろ']\n",
      "Can't write : ['a2WEYQY', 'Hii❤', 1, 'hii']\n",
      "Can't write : ['a7WvRDr', 'Kĕniæl, the Overseer', 2, 'overseer', 'kĕniæl']\n",
      "Can't write : ['aBm9Adx', \"ಠ_ಠ...When I read this news.I don't think they're UFOs...ಠ_ಠ\", 5, 'ಠ_ಠ', 'ufo', 'read', 'news', 'think']\n",
      "Can't write : ['aD4RNr9', 'Friends furever ♡', 2, 'furever', 'friend']\n",
      "Can't write : ['aGdKm87', 'ステイホーム', 1, 'ステイホーム']\n",
      "Can't write : ['aj9Qnr8', '¯\\\\_(ツ)_/¯ Well', 1, 'well']\n",
      "Can't write : ['an4ePOL', 'Well ¯\\\\_(ツ)_/¯', 1, 'well']\n",
      "Can't write : ['aNgm7Qv', 'Quarintine ️', 1, 'quarintine']\n",
      "Can't write : ['aqn40Lp', 'The new suprime leader is kinda cute ❤', 5, 'suprime', 'kinda', 'cute', 'leader', 'new']\n",
      "Can't write : ['aXgZ6dD', 'ᵒʰ ⁿᵒ', 2, 'ⁿᵒ', 'ᵒʰ']\n",
      "Can't write : ['aYyj0qN', 'The best of friends ♡', 2, 'friend', 'best']\n",
      "Most keywords : 29\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('keywords.csv', 'w', newline='') as keywords_file:\n",
    "    \n",
    "    writer = csv.writer(keywords_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)  \n",
    "    \n",
    "    sql_query = \"SELECT id, title FROM actual_post\"\n",
    "\n",
    "    df_titles=pd.read_sql(sql_query, connection)\n",
    "    \n",
    "    most_keywords = 0\n",
    "    \n",
    "    for index, row in df_titles.iterrows():\n",
    "        # print(row['id'], row['title'])\n",
    "        \n",
    "        title = row['title']\n",
    "\n",
    "        lemmatized_title = preprocess_lemmatize(title)\n",
    "\n",
    "        keywords = extract_keywords_tfidf(title, lemmatized_title)\n",
    "        \n",
    "        if len(keywords) > most_keywords:\n",
    "            most_keywords = len(keywords)\n",
    "        \n",
    "        write_data = [row['id']]\n",
    "        write_data.append(row['title'])\n",
    "        write_data.append(len(keywords))\n",
    "        write_data.extend(keywords)\n",
    "        \n",
    "        try: \n",
    "            writer.writerow(write_data)\n",
    "        except:\n",
    "            print(\"Can't write :\", write_data)\n",
    "        \n",
    "    print(\"Most keywords :\", most_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            Hat trick\n",
       "1                                       What happened?\n",
       "2                           Do you miss the Neinties ?\n",
       "3                              A different perspective\n",
       "4              One of the worst moments of pc building\n",
       "                             ...                      \n",
       "6033           Doomguy teaches Isabelle his techniques\n",
       "6034                                   Clear lemon pie\n",
       "6035    This entire thing was made with color pencils!\n",
       "6036                       Quarantine hacks gone wrong\n",
       "6037                     Dunder-Mifflin this is Brazil\n",
       "Name: title, Length: 6038, dtype: object"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_titles['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
